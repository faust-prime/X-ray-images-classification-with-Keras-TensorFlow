{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHK6DyunSbs4"
   },
   "source": [
    "# Cat vs. Dog Image Classification\n",
    "## Feature Extraction and Fine-Tuning\n",
    "**_Estimated completion time: 30 minutes_**\n",
    "\n",
    "In Exercise 1, we built a convnet from scratch, and were able to achieve an accuracy of about 70%. With the addition of data augmentation and dropout in Exercise 2, we were able to increase accuracy to about 80%. That seems decent, but 20% is still too high of an error rate. Maybe we just don't have enough training data available to properly solve the problem. What other approaches can we try?\n",
    "\n",
    "In this exercise, we'll look at two techniques for repurposing feature data generated from image models that have already been trained on large sets of data, **feature extraction** and **fine tuning**, and use them to improve the accuracy of our cat vs. dog classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dI5rmt4UBwXs"
   },
   "source": [
    "## Feature Extraction Using a Pretrained Model\n",
    "\n",
    "One thing that is commonly done in computer vision is to take a model trained on a very large dataset, run it on your own, smaller dataset, and extract the intermediate representations (features) that the model generates. These representations are frequently informative for your own computer vision task, even though the task may be quite different from the problem that the original model was trained on. This versatility and repurposability of convnets is one of the most interesting aspects of deep learning.\n",
    "\n",
    "In our case, we will use the [Inception V3 model](https://arxiv.org/abs/1512.00567) developed at Google, and pre-trained on [ImageNet](http://image-net.org/), a large dataset of web images (1.4M images and 1000 classes). This is a powerful model; let's see what the features that it has learned can do for our cat vs. dog problem.\n",
    "\n",
    "First, we need to pick which intermediate layer of Inception V3 we will use for feature extraction. A common practice is to use the output of the very last layer before the `Flatten` operation, the so-called \"bottleneck layer.\" The reasoning here is that the following fully connected layers will be too specialized for the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\n",
    "\n",
    "Let's instantiate an Inception V3 model preloaded with weights trained on ImageNet:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcYZPBS3bTAj"
   },
   "source": [
    "By specifying the `include_top=False` argument, we load a network that doesn't include the classification layers at the top—ideal for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFxrqTuJee5m"
   },
   "source": [
    "Let's make the model non-trainable, since we will only use it for feature extraction; we won't update the weights of the pretrained model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGBGDiOAepnO"
   },
   "source": [
    "The layer we will use for feature extraction in Inception v3 is called `mixed7`. It is not the bottleneck of the network, but we are using it to keep a sufficiently large feature map (7x7 in this case). (Using the bottleneck layer would have resulting in a 3x3 feature map, which is a bit small.) Let's get the output from `mixed7`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxHk6XQLeUWh"
   },
   "source": [
    "Now let's stick a fully connected classifier on top of `last_output`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMXb913pbvFg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34283 images belonging to 15 classes.\n",
      "Found 8564 images belonging to 15 classes.\n",
      "total Atelectasis images:        2015\n",
      "total Cardiomegaly images:       464\n",
      "total Consolidation images:      633\n",
      "total Edema images:              310\n",
      "total Effusion images:           1804\n",
      "total Emphysema images:          461\n",
      "total Fibrosis images:           366\n",
      "total Hernia images:             53\n",
      "total Infiltration images:       4640\n",
      "total Mass images:               1043\n",
      "total No_Finding images:         28000\n",
      "total Nodule images:             1273\n",
      "total Pleural_Thickening images: 525\n",
      "total Pneumonia images:          161\n",
      "total Pneumothorax images:       1099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 100s 1us/step\n",
      "Epoch 1/30\n",
      "1714/1714 [==============================] - 578s 337ms/step - loss: 1.5256 - val_loss: 1.5274\n",
      "Epoch 2/30\n",
      "1714/1714 [==============================] - 570s 333ms/step - loss: 1.3810 - val_loss: 1.7968\n",
      "Epoch 3/30\n",
      "1714/1714 [==============================] - 571s 333ms/step - loss: 1.3690 - val_loss: 1.5211\n",
      "Epoch 4/30\n",
      "1714/1714 [==============================] - 572s 334ms/step - loss: 1.3670 - val_loss: 1.6591\n",
      "Epoch 5/30\n",
      "1714/1714 [==============================] - 568s 332ms/step - loss: 1.3639 - val_loss: 1.6433\n",
      "Epoch 6/30\n",
      "1714/1714 [==============================] - 568s 331ms/step - loss: 1.3623 - val_loss: 1.8863\n",
      "Epoch 7/30\n",
      "1714/1714 [==============================] - 568s 331ms/step - loss: 1.3660 - val_loss: 1.6271\n",
      "Epoch 8/30\n",
      "1713/1714 [============================>.] - ETA: 0s - loss: 1.3667"
     ]
    }
   ],
   "source": [
    "base_dir = '/media/ente/M2/2018 - 11 - sorted data'\n",
    "class_names = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax' ]\n",
    "class_mode = 'categorical'\n",
    "batch_size = 20\n",
    "epochs = 30\n",
    "color_mode='rgb'\n",
    "\n",
    "#class dirs\n",
    "import os\n",
    "ate_dir = os.path.join(base_dir, class_names[0])\n",
    "car_dir = os.path.join(base_dir, class_names[1])\n",
    "con_dir = os.path.join(base_dir, class_names[2])\n",
    "ede_dir = os.path.join(base_dir, class_names[3])\n",
    "eff_dir = os.path.join(base_dir, class_names[4])\n",
    "emp_dir = os.path.join(base_dir, class_names[5])\n",
    "fib_dir = os.path.join(base_dir, class_names[6])\n",
    "her_dir = os.path.join(base_dir, class_names[7])\n",
    "inf_dir = os.path.join(base_dir, class_names[8])\n",
    "mas_dir = os.path.join(base_dir, class_names[9])\n",
    "nof_dir = os.path.join(base_dir, class_names[10])\n",
    "nod_dir = os.path.join(base_dir, class_names[11])\n",
    "ple_dir = os.path.join(base_dir, class_names[12])\n",
    "pne_dir = os.path.join(base_dir, class_names[13])\n",
    "pn2_dir = os.path.join(base_dir, class_names[14])\n",
    "#filenames\n",
    "ate_fnames = os.listdir(ate_dir)\n",
    "car_fnames = os.listdir(car_dir)\n",
    "con_fnames = os.listdir(con_dir)\n",
    "ede_fnames = os.listdir(ede_dir)\n",
    "eff_fnames = os.listdir(eff_dir)\n",
    "emp_fnames = os.listdir(emp_dir)\n",
    "fib_fnames = os.listdir(fib_dir)\n",
    "her_fnames = os.listdir(her_dir)\n",
    "inf_fnames = os.listdir(inf_dir)\n",
    "mas_fnames = os.listdir(mas_dir)\n",
    "nof_fnames = os.listdir(nof_dir)\n",
    "nod_fnames = os.listdir(nod_dir)\n",
    "ple_fnames = os.listdir(ple_dir)\n",
    "pne_fnames = os.listdir(pne_dir)\n",
    "pn2_fnames = os.listdir(pn2_dir)\n",
    "#print (train_ate_fnames[:10])\n",
    "#train_car_fnames.sort()\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    #zoom_range=zoom_range,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    #target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode,\n",
    "    color_mode=color_mode, \n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    base_dir, # same directory as training data\n",
    "    #target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode, #Determines the type of label arrays that are returned:\"categorical\" will be 2D one-hot encoded labels,\n",
    "    color_mode=color_mode, \n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "print ('total Atelectasis images:       ', len(os.listdir(ate_dir))) \n",
    "print ('total Cardiomegaly images:      ', len(os.listdir(car_dir))) \n",
    "print ('total Consolidation images:     ', len(os.listdir(con_dir))) \n",
    "print ('total Edema images:             ', len(os.listdir(ede_dir))) \n",
    "print ('total Effusion images:          ', len(os.listdir(eff_dir))) \n",
    "print ('total Emphysema images:         ', len(os.listdir(emp_dir))) \n",
    "print ('total Fibrosis images:          ', len(os.listdir(fib_dir))) \n",
    "print ('total Hernia images:            ', len(os.listdir(her_dir))) \n",
    "print ('total Infiltration images:      ', len(os.listdir(inf_dir))) \n",
    "print ('total Mass images:              ', len(os.listdir(mas_dir))) \n",
    "print ('total No_Finding images:        ', len(os.listdir(nof_dir))) \n",
    "print ('total Nodule images:            ', len(os.listdir(nod_dir))) \n",
    "print ('total Pleural_Thickening images:', len(os.listdir(ple_dir))) \n",
    "print ('total Pneumonia images:         ', len(os.listdir(pne_dir))) \n",
    "print ('total Pneumothorax images:      ', len(os.listdir(pn2_dir))) \n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(len(class_names), activation = 'softmax') (x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "#model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "####################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "#grid_result = grid.fit(X,Y)\n",
    "grid_result = grid.fit_generator(    \n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = epochs)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "########################\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "#####model.fit_generator(    \n",
    "    #train_generator,\n",
    "    #steps_per_epoch = train_generator.samples // batch_size,\n",
    "    #validation_data = validation_generator,\n",
    "    #validation_steps = validation_generator.samples // batch_size,\n",
    "    #epochs = epochs)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "######from keras.optimizers import SGD\n",
    "######model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(    \n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6ECjowwV5Ug"
   },
   "source": [
    "## data preprocessing + train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRjyAkE62aOG"
   },
   "source": [
    "You can see that we reach a validation accuracy of 88–90% very quickly. This is much better than the small model we trained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tt15y6IS2pBo"
   },
   "source": [
    "## Further Improving Accuracy with Fine-Tuning\n",
    "\n",
    "In our feature-extraction experiment, we only tried adding two classification layers on top of an Inception V3 layer. The weights of the pretrained network were not updated during training. One way to increase performance even further is to \"fine-tune\" the weights of the top layers of the pretrained model alongside the training of the top-level classifier. A couple of important notes on fine-tuning:\n",
    "\n",
    "- **Fine-tuning should only be attempted *after* you have trained the top-level classifier with the pretrained model set to non-trainable**. If you add a randomly initialized classifier on top of a pretrained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier), and your pretrained model will just forget everything it has learned.\n",
    "- Additionally, we **fine-tune only the *top layers* of the pre-trained model** rather than all layers of the pretrained model because, in a convnet, the higher up a layer is, the more specialized it is. The first few layers in a convnet learn very simple and generic features, which generalize to almost all types of images. But as you go higher up, the features are increasingly specific to the dataset that the model is trained on. The goal of fine-tuning is to adapt these specialized features to work with the new dataset.\n",
    "\n",
    "All we need to do to implement fine-tuning is to set the top layers of Inception V3 to be trainable, recompile the model (necessary for these changes to take effect), and resume training. Let's unfreeze all layers belonging to the `mixed7` module—i.e., all layers found after `mixed6`—and recompile the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE37ARlqY9da"
   },
   "source": [
    "Now let's retrain the model. We'll train on all 2000 images available, for 50 epochs, and validate on all 1,000 test images. (This may take 15-20 minutes to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_GgDGG4Y_hJ"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3EPGn58ofwq5"
   },
   "source": [
    "We are seeing a nice improvement, with the validation loss going from ~1.7 down to ~1.2, and accuracy going from 88% to 92%. That's a 4.5% relative improvement in accuracy.\n",
    "\n",
    "Let's plot the training and validation loss and accuracy to show it conclusively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FtxcKjJfxL9"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Retrieve a list of accuracy results on training and test data\n",
    "# sets for each training epoch\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-fUIeizakjE"
   },
   "source": [
    "Congratulations! Using feature extraction and fine-tuning, you've built an image classification model that can identify cats vs. dogs in images with over 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_ANwJCnx7w-"
   },
   "source": [
    "## Clean Up\n",
    "\n",
    "Run the following cell to terminate the kernel and free memory resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hUmyohAyBzh"
   },
   "outputs": [],
   "source": [
    "#import os, signal\n",
    "#os.kill(os.getpid(), signal.SIGKILL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jTEzoMx6CasV"
   ],
   "name": "image_classification_part3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
